{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reg_utils.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTvEw7i5hB/4R0lqbeDFDI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thapaliya123/cat_dog_predictions/blob/master/reg_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eOS3fj6GBGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import  pyplot as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "#loading_feature_and_labels_from_created_pickle_file\n",
        "def load_datasets(feature_path, labels_path):\n",
        "  \n",
        "  pickle_in = open(\"{}\".format(feature_path), \"rb\")\n",
        "  X_train = pickle.load(pickle_in)\n",
        "  X_train = X_train/255.0\n",
        "\n",
        "  pickle_in = open(\"{}\".format(labels_path),\"rb\")\n",
        "  Y_train = pickle.load(pickle_in)\n",
        "\n",
        "  return X_train, Y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbC5H0bcK_9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initializing the parameters\n",
        "def initialize_paramters_deep(layers_dims):\n",
        "  '''\n",
        "  Arguments:\n",
        "  layer_dims -- python array(list) containing the dimension of each layer in our network\n",
        "\n",
        "  Returns:\n",
        "  parameters -- python dictionary containing your parameters \"W1\", \"b1\",..\"WL\", \"bL\"\n",
        "  WL -- Weight matrix of shape(layer_dims[L], layer_dims[L-1])\n",
        "  bL -- bias vector of shape(layer_dims[L], 1)\n",
        "  '''\n",
        "  np.random.seed(3)\n",
        "  parameters = {}\n",
        "  L = len(layers_dims)\n",
        "\n",
        "  for l in range(1, L):\n",
        "    parameters[\"W\"+str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2/layers_dims[l-1])#He_initialization_pf_parameters\n",
        "    parameters[\"b\"+str(l)] = np.zeros((layers_dims[l], 1))\n",
        "  \n",
        "  return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B8WmpbkMHvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#forward_propagation_module\n",
        "\n",
        "#implementation_of_linear_activation\n",
        "def linear_forward(A, W, b):\n",
        "  '''\n",
        "  Implement the linear part of the layers forward propagation\n",
        "\n",
        "  Arguments:\n",
        "  A -- activation from the previous layers(or input):(size_of_prev_layer, no_of_examples)\n",
        "  W -- Weight matrix:numpy array of shape(size_of_current_layer, size_of_prev_layer)\n",
        "  b -- bias vector:numpy array of shape(size_of_current_layer, 1)\n",
        "  \n",
        "  Returns:/\n",
        "  Z -- the input of the activation functions, also called pre-activation parameters\n",
        "  cache -- a python dictionary containing \"A\", \"W\", \"b\" stored for computing the backward pass efficiently\n",
        "  '''\n",
        "  Z = np.dot(W, A)+b\n",
        "\n",
        "  assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "\n",
        "  cache = A, W, b\n",
        "  \n",
        "  \n",
        "  return  Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vta5yER8ObPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#implementation_of_sigmoid_activation_function\n",
        "def  sigmoid(Z):\n",
        "  '''\n",
        "  Implements the sigmoid activation in numpy\n",
        "\n",
        "  Arguments:\n",
        "  Z -- numpy array of any shape\n",
        "\n",
        "  Returns:\n",
        "  A -- output of the sigmoid(Z) same shape as that of Z\n",
        "  cache -- returns Z as well usefull during the backpropagation\n",
        "  '''\n",
        "  A = 1/(1+np.exp(-Z))\n",
        "  cache = Z\n",
        "\n",
        "  return A, cache\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24ymS9IKO0pc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#implementation_relu_activation_function\n",
        "\n",
        "def relu(Z):\n",
        "  '''\n",
        "  Implements the relu activation function\n",
        "\n",
        "  Arguments:\n",
        "  Z -- Output of the linear layer of any shape\n",
        "\n",
        "  Returns:\n",
        "  A -- output of relu(Z) same shape as that of Z\n",
        "  cache -- returns Z as activation_cache as well usefull during the backpropagation\n",
        "  '''\n",
        "  A = np.maximum(0,Z)\n",
        "  cache = Z\n",
        "\n",
        "  return A,cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXfEHioDO3f2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#implement_linear_activation_forward\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation, keep_prob):\n",
        "  '''\n",
        "  Implements forward propagation for linear->activation layer\n",
        "\n",
        "  Arguments:\n",
        "  A_prev -- Activation from the previous layer\n",
        "  W -- Weight matrix of current layer of shape(size_of_current_layer, size_of_previous_layer)\n",
        "  b -- bias vector:numpy array of shape(size_of_current_layer, 1)\n",
        "  activation -- activation function to be used either sigmoid or relu \n",
        "  '''\n",
        "  global cache\n",
        "  global linear_cache\n",
        "  global activation_cache\n",
        "  np.random.seed(1)\n",
        "  if activation == 'sigmoid':\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    A, activation_cache = sigmoid(Z)\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "  elif activation == 'relu':\n",
        "    if(keep_prob<1):\n",
        "      Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "      A, activation_cache = relu(Z)\n",
        "      D = np.random.rand(A.shape[0], A.shape[1]) #making_same_shape_as_that_of_A\n",
        "      D = (D<keep_prob).astype(int) #shut_neuron_greater_than_keep_prob\n",
        "      A = A*D #shut_neuron\n",
        "      A = A/keep_prob\n",
        "      cache = (linear_cache, D, activation_cache)\n",
        "\n",
        "\n",
        "    elif(keep_prob==1):\n",
        "      Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "      A, activation_cache = relu(Z)\n",
        "      cache = (linear_cache, activation_cache)\n",
        "\n",
        "    \n",
        "    else:\n",
        "      pass\n",
        "    \n",
        "  else:\n",
        "    pass\n",
        "    \n",
        "  # assert(A.shape == (W.shape[0], A.shape[1]))\n",
        "  return A, cache\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfIfotESO7g5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_forward(X, parameters, keep_prob=1):\n",
        "  '''\n",
        "  Implement forward propagation for the [Linear->Relu]*(L-1) -> Linear->Sigmoid computation\n",
        "\n",
        "  Arguments:\n",
        "  X -- data, numpy array of shape(input_size, number_of_examples)\n",
        "  parameters -- output of initialize_parameters_deep()\n",
        "\n",
        "  Returns:\n",
        "  AL -- last post_activation_value\n",
        "  caches -- list of cache containing every cache containing:every cache of linear_relu_forward()\n",
        "   -(there are L-1 of them, indexed from 0 to L-2)\n",
        "   -the cache of linear_sigmoid_forward()(there is one, indexed L-1)\n",
        "  '''\n",
        "  caches = []\n",
        "  A = X\n",
        "  L = len(parameters)//2 #numbers of layers in the network\n",
        "  \n",
        "  for l in range(1, L):\n",
        "    A_prev = A\n",
        "    A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], 'relu', keep_prob)\n",
        "    caches.append(cache)\n",
        "  \n",
        "  AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], 'sigmoid', keep_prob)\n",
        "  caches.append(cache)\n",
        "\n",
        "  assert(AL.shape == (1,X.shape[1]))\n",
        "\n",
        "  return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9wEJSPlO_y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compute_cost\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "  '''\n",
        "  Implements the cost function defined by crossentropy \n",
        "\n",
        "  Arguments:\n",
        "  AL -- probability vector corresponding to your label predictions, shape(1, number_of_examples)\n",
        "  Y -- True labels vector for examples containing 0 if cat and 1 if dog, shape(1, number_of_examples)\n",
        "\n",
        "  Returns:\n",
        "  cost -- crossentropy cost\n",
        "  '''\n",
        "\n",
        "  m = Y.shape[1]\n",
        "\n",
        "  #compute cost from AL and Y\n",
        "  logprobs = np.multiply(np.log(AL), Y) + np.multiply(np.log(1-AL), 1-Y)\n",
        "  cost = - (1/m)*np.sum(logprobs)\n",
        "\n",
        "  assert(cost.shape == ())\n",
        "\n",
        "  return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7XHhbUa_5hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Regularizations\n",
        "def compute_cost_with_regularization(AL, Y, parameters, lambd):\n",
        "  '''\n",
        "  Implement_the_cost_with_L2_regularization\n",
        "\n",
        "  Arguments:\n",
        "  AL -- post activation, output of forward propagation of shape(output_size, number_of_example)\n",
        "  Y -- \"true\" labels vector of shape(output_size, number_of_examples)\n",
        "  parameters -- python Dictionary containing your parameters\n",
        "  lambd -- L2 Regularization\n",
        "\n",
        "  Returns:\n",
        "  cost -- value of the regularized cost functions\n",
        "  '''\n",
        "\n",
        "  m = Y.shape[1]\n",
        "  L = len(parameters)//2 #number_of_layers_in_the_network\n",
        "  L2_regularization_cost = 0\n",
        "  cross_entropy_cost = compute_cost(AL, Y)\n",
        "  for l in range(1, L+1):\n",
        "    L2_regularization_cost = L2_regularization_cost + ((lambd/(2*m))*(np.sum(np.square(parameters[\"W\"+str(l)]))))\n",
        "  \n",
        "  cost = cross_entropy_cost+L2_regularization_cost\n",
        "\n",
        "  return cost\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD7EhcY2PDQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#implements_relu_backward_function\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    dZ[Z<=0] = 0\n",
        "\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taQ4RWxkPKT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_backward(dA, cache):\n",
        "  '''\n",
        "  Implements the backward propagation for the single sigmoid unit\n",
        "\n",
        "  Arguments:\n",
        "  dA -- post activation gradients,of any shape\n",
        "  cache -- \"Z\" where we store for computing backward propagation more efficiently\n",
        "\n",
        "  Returns:\n",
        "  dZ -- Gradient of the cost with respect to Z\n",
        "  '''\n",
        "  Z = cache\n",
        "  s = 1/(1+np.exp(-Z))\n",
        "\n",
        "  dZ = dA*s*(1-s)\n",
        "\n",
        "  assert(dZ.shape == Z.shape)\n",
        "  return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8NewTppPOIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#backward_propagation\n",
        "\n",
        "#linear_backward\n",
        "\n",
        "def linear_backward(dZ, cache, lambd):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "    if(lambd == 0):\n",
        "      dW = (1./m)*(np.dot(dZ, A_prev.T))\n",
        "    \n",
        "    else:\n",
        "      dW = ((1./m)*(np.dot(dZ, A_prev.T)))+((lambd*W)/m)\n",
        "    \n",
        "    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp5u016_PQQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#linear_activation_backward\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation, lambd, keep_prob):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    global linear_cache\n",
        "    global activation_cache\n",
        "    global D\n",
        "    if activation == \"relu\":\n",
        "      if(keep_prob==1):\n",
        "        linear_cache, activation_cache = cache\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache, lambd)\n",
        "      \n",
        "      elif(keep_prob<1):\n",
        "        linear_cache, D, activation_cache = cache\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache, lambd)\n",
        "        # dA_prev = dA_prev*D #dropping_out_same_neuron_that_was_dropped_in_forward_propagation\n",
        "        # dA_prev = dA_prev/keep_prob #scaling_the_remaining_neuron_by_keep_prob\n",
        "      \n",
        "      else:\n",
        "        pass\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        linear_cache, activation_cache = cache\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache, lambd)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWL7IpSQPSXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#backpropagate_through_network\n",
        "def L_model_backward(AL, Y, caches, lambd=0, keep_prob=1):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group, if value of lambd is passed the it will apply L2 regularization\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    lambd -- L2 regularizing parameter if value is passed then it will apply L2 regularization\n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = np.array(Y)\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    ### START CODE HERE ### (1 line of code)\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid', lambd, keep_prob)\n",
        "    # if(keep_prob<1):\n",
        "    #   l_cache, D, a_cache = caches[L-2]\n",
        "    #   grads[\"dA\" + str(L-1)] = grads[\"dA\" + str(L-1)]*D #dropping_out_the_neuron\n",
        "    #   grads[\"dA\" + str(L-1)] = grads[\"dA\" + str(L-1)]/keep_prob #scaling_neuron_with_the_keep_prob_value\n",
        "\n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "        current_cache = caches[l]\n",
        "        if(keep_prob<1):\n",
        "          l_cache, D, a_cache = current_cache\n",
        "          grads[\"dA\"+str(l+1)] = grads[\"dA\"+str(l+1)]*D\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, 'relu', lambd, keep_prob)\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wAC5gTGQSY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predictions for binary classifier\n",
        "\n",
        "def predict(parameters, X):\n",
        "  '''\n",
        "  Using the learned parameters,predicts the class for each example\n",
        "\n",
        "  Arguments:\n",
        "  parameters -- python dictionary containing your learned parameters\n",
        "  X -- input data of size(n_x, m)\n",
        "\n",
        "  Return:\n",
        "  predictions -- vector of predictions of our model(cat:0/dog:1)\n",
        "  '''\n",
        "\n",
        "  AL, cache = L_model_forward(X, parameters)\n",
        "\n",
        "  for i in range(AL.shape[1]):\n",
        "    if(AL[0,i] <= 0.5):\n",
        "      AL[0, i] = 0\n",
        "    else:\n",
        "      AL[0,i] = 1\n",
        "  \n",
        "  return AL"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}