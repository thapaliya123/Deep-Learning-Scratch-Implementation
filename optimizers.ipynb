{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimizers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvlFGYVghmHFQHmlNHOcAG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thapaliya123/cat_dog_predictions/blob/master/optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyTGbpry9vKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gradient_descent_update_rule\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\"+str(l+1)] - learning_rate*grads[\"dW\"+str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\"+str(l+1)] - learning_rate*grads[\"db\"+str(l+1)]\n",
        "    ### END CODE HERE ###\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-jWEU8i-5XK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "#initialization_for_adam_optimization\n",
        "def initialize_adam(parameters):\n",
        "  '''\n",
        "  Initializes v and s two python dictionaries with key's.\n",
        "    -keys:\"dW1\",\"db1\"........\"dWL\",\"dbL\"\n",
        "    -values:numpy areas initializes with zeros with same shape as that of respective gradients/parameters\n",
        "\n",
        "  Arguments:\n",
        "  parameters -- python dictionary containin your parameters.\n",
        "\n",
        "  Returns:\n",
        "  v -- python dictionary that will contain your exponentially weighted average of the gradients\n",
        "    v[\"dW\"+str(l)]=....\n",
        "    v[\"db\"+str(l)]=....\n",
        "\n",
        "  s -- python dictionary that will contain the exponentially weighted average of the square of the gradients\n",
        "    s[\"dW\"+str(l)]=....\n",
        "    s[\"db\"+str(l)]=....\n",
        "  '''\n",
        "  L = len(parameters)//2 #number_of_layers_in_the_model\n",
        "  v = {}\n",
        "  s = {}\n",
        "  for l in range(1,L+1):\n",
        "    v[\"dW\" + str(l)] = np.zeros((parameters[\"W\"+str(l)].shape[0], parameters[\"W\"+str(l)].shape[1]))\n",
        "    v[\"db\" + str(l)] = np.zeros((parameters[\"b\"+str(l)].shape[0], parameters[\"b\"+str(l)].shape[1]))\n",
        "    s[\"dW\" + str(l)] = np.zeros((parameters[\"W\"+str(l)].shape[0], parameters[\"W\"+str(l)].shape[1]))\n",
        "    s[\"db\" + str(l)] = np.zeros((parameters[\"b\"+str(l)].shape[0], parameters[\"b\"+str(l)].shape[1]))\n",
        "  return v, s\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl5X00ds_KvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#adam_optimizer_update_rules\n",
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "  '''\n",
        "  update parameters using Adam\n",
        "\n",
        "  Arguments:\n",
        "  parameters -- python dictionary containing your parameters\n",
        "  grads -- python dictionary containing your gradients\n",
        "  v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "  s -- Adam variable, moving gradients of the square gradient, python dictionary\n",
        "  t -- epoch_num\n",
        "  learning_rate -- the learning_rate scalar.\n",
        "  beta1 -- Exponentially decay hyperparameter for the first moment estimates\n",
        "  beta2 -- Exponentially decay hyperparameter for the second moment estimates\n",
        "  epsilon -- hyperparameter preventing division by zeros in the Adam updates\n",
        "\n",
        "  Returns:\n",
        "  parameters -- python dictionary containing your updated parameters\n",
        "  v -- Adam variable, moving gradients of the first quadrant, python dictionary\n",
        "  s -- Adam variable, moving gradients of the second quadrant, python dictionary\n",
        "  '''\n",
        "  L = len(parameters)//2 #number of Layers in the model\n",
        "  v_corrected = {} #initializing first moment estimates, python dictionary\n",
        "  s_corrected = {} #initializing second moment estimates, python dictionary\n",
        "  for l in range(L):\n",
        "    # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        "    v[\"dW\" + str(l+1)] = (beta1*v[\"dW\"+str(l+1)]) + ((1-beta1)*grads['dW'+str(l+1)])\n",
        "    v[\"db\" + str(l+1)] = (beta1*v[\"db\"+str(l+1)]) + ((1-beta1)*grads['db'+str(l+1)])\n",
        "    \n",
        "    # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
        "    v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
        "    v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
        "\n",
        "    # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "    s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n",
        "    s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n",
        "\n",
        "    # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "    s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
        "    s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
        "\n",
        "    # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".+ epsilon\n",
        "    parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * (v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)]+epsilon))\n",
        "    parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * (v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)]+epsilon))\n",
        "  return parameters, v, s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_UCU9r8CgGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}